{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoFpNP4mM7EC+mAdIu5cbE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nebuchad-nezzar/Python-Notebooks/blob/main/Transfer_Learningipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        },
        "id": "klA6kBZR7UIO",
        "outputId": "d471c7d9-43d3-4004-e522-0d7b51fea8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using device: cpu\n",
            "Loading dataset...\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 39.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Creating data loaders...\n",
            "Initializing model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 107MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|          | 9/938 [01:15<2:09:08,  8.34s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bf3ed27a2974>\u001b[0m in \u001b[0;36m<cell line: 191>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-bf3ed27a2974>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Show some predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bf3ed27a2974>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torch torchvision tqdm matplotlib\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.models import resnet18  # Using ResNet18 as it's lighter\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Check if GPU is available (important for Colab)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # ResNet expects 224x224 images\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "def load_dataset():\n",
        "    # Download and load training set\n",
        "    full_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Split into train, validation, and test (60%, 20%, 20%)\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(0.6 * total_size)\n",
        "    val_size = int(0.2 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "# Create data loaders\n",
        "def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size=32):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Define the model\n",
        "def load_model():\n",
        "    # Load pre-trained ResNet18\n",
        "    model = resnet18(pretrained=True)\n",
        "\n",
        "    # Modify the final layer for CIFAR-10 (10 classes)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Lists to store metrics for plotting\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Training loop with progress bar\n",
        "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss for this epoch\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}:')\n",
        "        print(f'Training Loss: {epoch_loss:.3f}')\n",
        "        print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Save best model\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    # Plot training progress\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(val_accuracies)\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.show()\n",
        "\n",
        "# Function to show predictions\n",
        "def show_predictions(model, test_loader, classes, num_images=5):\n",
        "    model.eval()\n",
        "\n",
        "    # Get a batch of test images\n",
        "    dataiter = iter(test_loader)\n",
        "    images, labels = next(dataiter)\n",
        "    images = images[:num_images]\n",
        "    labels = labels[:num_images]\n",
        "\n",
        "    # Get predictions\n",
        "    images = images.to(device)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Show images and predictions\n",
        "    fig = plt.figure(figsize=(15, 3))\n",
        "    for idx in range(num_images):\n",
        "        ax = plt.subplot(1, num_images, idx + 1)\n",
        "        img = images[idx].cpu().numpy().transpose((1, 2, 0))\n",
        "        img = img * 0.5 + 0.5  # Denormalize\n",
        "        plt.imshow(img)\n",
        "        color = 'green' if predicted[idx] == labels[idx] else 'red'\n",
        "        ax.set_title(f'Pred: {classes[predicted[idx]]}\\nTrue: {classes[labels[idx]]}',\n",
        "                    color=color)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # CIFAR-10 classes\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "              'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    # Load and split dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    train_dataset, val_dataset, test_dataset = load_dataset()\n",
        "\n",
        "    # Create data loaders\n",
        "    print(\"Creating data loaders...\")\n",
        "    train_loader, val_loader, test_loader = create_data_loaders(\n",
        "        train_dataset, val_dataset, test_dataset\n",
        "    )\n",
        "\n",
        "    # Initialize model and training components\n",
        "    print(\"Initializing model...\")\n",
        "    model = load_model()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train model\n",
        "    print(\"Starting training...\")\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "    # Show some predictions\n",
        "    print(\"\\nShowing predictions...\")\n",
        "    show_predictions(model, test_loader, classes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet Training Pipeline with Anti-Overfitting Techniques"
      ],
      "metadata": {
        "id": "w8koNIZg9xxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class EnhancedResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(EnhancedResNet, self).__init__()\n",
        "        # Load pretrained ResNet\n",
        "        self.resnet = resnet18(pretrained=True)\n",
        "\n",
        "        # Remove the final fully connected layer\n",
        "        num_features = self.resnet.fc.in_features\n",
        "\n",
        "        # Add dropout and new classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),  # Add dropout with 0.5 probability\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  # Add another dropout layer\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        self.resnet.fc = self.classifier\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Enhanced data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Validation transform (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "def train_model_enhanced(model, train_loader, val_loader, criterion, optimizer,\n",
        "                        scheduler, num_epochs=30, device='cuda'):\n",
        "    early_stopping = EarlyStopping(patience=5)\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with dropout enabled (model.train() enables dropout)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Add L2 regularization loss\n",
        "            l2_lambda = 0.01\n",
        "            l2_reg = torch.tensor(0.).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg += torch.norm(param)\n",
        "            loss += l2_lambda * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_train_loss = train_loss / len(train_loader)\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_train_acc = 100. * train_correct / train_total\n",
        "        epoch_val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Store history\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%')\n",
        "        print(f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
        "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(epoch_val_loss)\n",
        "        if early_stopping.should_stop:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    ax1.plot(history['train_loss'], label='Training Loss')\n",
        "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title('Model Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    ax2.plot(history['train_acc'], label='Training Accuracy')\n",
        "    ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    ax2.set_title('Model Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load datasets with different transforms for train and validation\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=train_transform\n",
        "    )\n",
        "    val_size = 5000\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = EnhancedResNet().to(device)\n",
        "\n",
        "    # Initialize training components\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    # Train model\n",
        "    history = train_model_enhanced(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3LqlMNi7VHl",
        "outputId": "cebddf22-5aa3-4a41-d388-37878b92e1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 102MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1/30: 100%|██████████| 1407/1407 [05:26<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30:\n",
            "Train Loss: 4.7061, Train Acc: 32.60%\n",
            "Val Loss: 1.8388, Val Acc: 32.02%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 1407/1407 [05:27<00:00,  4.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/30:\n",
            "Train Loss: 2.9094, Train Acc: 34.60%\n",
            "Val Loss: 1.9309, Val Acc: 28.82%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 1407/1407 [05:19<00:00,  4.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/30:\n",
            "Train Loss: 2.4537, Train Acc: 38.28%\n",
            "Val Loss: 1.6342, Val Acc: 39.10%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 1407/1407 [05:17<00:00,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/30:\n",
            "Train Loss: 2.2566, Train Acc: 40.09%\n",
            "Val Loss: 1.6213, Val Acc: 39.86%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 1407/1407 [05:17<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/30:\n",
            "Train Loss: 2.1525, Train Acc: 42.11%\n",
            "Val Loss: 1.5871, Val Acc: 41.78%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 1407/1407 [05:15<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/30:\n",
            "Train Loss: 2.1093, Train Acc: 44.03%\n",
            "Val Loss: 1.5521, Val Acc: 43.66%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 1407/1407 [05:13<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/30:\n",
            "Train Loss: 2.0713, Train Acc: 45.26%\n",
            "Val Loss: 1.9379, Val Acc: 32.46%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 1407/1407 [05:15<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/30:\n",
            "Train Loss: 2.0585, Train Acc: 45.85%\n",
            "Val Loss: 1.5567, Val Acc: 42.76%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 1407/1407 [05:16<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/30:\n",
            "Train Loss: 2.0510, Train Acc: 46.17%\n",
            "Val Loss: 1.5250, Val Acc: 44.06%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 1407/1407 [05:15<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/30:\n",
            "Train Loss: 2.0403, Train Acc: 46.18%\n",
            "Val Loss: 4.6024, Val Acc: 18.76%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 1407/1407 [05:13<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/30:\n",
            "Train Loss: 2.0229, Train Acc: 46.92%\n",
            "Val Loss: 1.6196, Val Acc: 42.10%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/30:\n",
            "Train Loss: 2.0207, Train Acc: 46.99%\n",
            "Val Loss: 1.9084, Val Acc: 36.14%\n",
            "Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 1407/1407 [05:15<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/30:\n",
            "Train Loss: 2.0281, Train Acc: 47.34%\n",
            "Val Loss: 1.5871, Val Acc: 40.76%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/30:\n",
            "Train Loss: 1.8604, Train Acc: 51.79%\n",
            "Val Loss: 1.2433, Val Acc: 55.14%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|██████████| 1407/1407 [05:12<00:00,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/30:\n",
            "Train Loss: 1.7859, Train Acc: 53.78%\n",
            "Val Loss: 1.2542, Val Acc: 54.34%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|██████████| 1407/1407 [05:12<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/30:\n",
            "Train Loss: 1.7531, Train Acc: 54.25%\n",
            "Val Loss: 1.2396, Val Acc: 55.46%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/30:\n",
            "Train Loss: 1.7310, Train Acc: 54.60%\n",
            "Val Loss: 1.2032, Val Acc: 56.50%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/30:\n",
            "Train Loss: 1.7213, Train Acc: 54.76%\n",
            "Val Loss: 1.1888, Val Acc: 57.34%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|██████████| 1407/1407 [05:15<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/30:\n",
            "Train Loss: 1.7076, Train Acc: 55.14%\n",
            "Val Loss: 1.2487, Val Acc: 55.34%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/30:\n",
            "Train Loss: 1.6958, Train Acc: 55.39%\n",
            "Val Loss: 1.1999, Val Acc: 57.64%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21/30:\n",
            "Train Loss: 1.6860, Train Acc: 55.27%\n",
            "Val Loss: 1.1946, Val Acc: 57.08%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22/30:\n",
            "Train Loss: 1.6735, Train Acc: 56.04%\n",
            "Val Loss: 1.1724, Val Acc: 57.70%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23/30:\n",
            "Train Loss: 1.6592, Train Acc: 56.30%\n",
            "Val Loss: 1.2155, Val Acc: 56.12%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|██████████| 1407/1407 [05:14<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24/30:\n",
            "Train Loss: 1.6518, Train Acc: 56.52%\n",
            "Val Loss: 1.1706, Val Acc: 57.94%\n",
            "Learning Rate: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30:  60%|█████▉    | 843/1407 [03:08<02:00,  4.70it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Analysis with Stopping Criteria Visualization"
      ],
      "metadata": {
        "id": "uvoMh8ik-MTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "class TrainingAnalyzer:\n",
        "    def __init__(self, history):\n",
        "        self.history = history\n",
        "        self.train_loss = history['train_loss']\n",
        "        self.val_loss = history['val_loss']\n",
        "        self.train_acc = history['train_acc']\n",
        "        self.val_acc = history['val_acc']\n",
        "        self.epochs = range(1, len(self.train_loss) + 1)\n",
        "\n",
        "    def plot_comprehensive_analysis(self):\n",
        "        \"\"\"Create a comprehensive visualization of training metrics\"\"\"\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # 1. Loss Curves with Smoothing\n",
        "        self._plot_smoothed_loss(ax1)\n",
        "\n",
        "        # 2. Training vs Validation Accuracy\n",
        "        self._plot_accuracy_comparison(ax2)\n",
        "\n",
        "        # 3. Generalization Gap\n",
        "        self._plot_generalization_gap(ax3)\n",
        "\n",
        "        # 4. Loss Gradient Analysis\n",
        "        self._plot_loss_gradient(ax4)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def _plot_smoothed_loss(self, ax):\n",
        "        \"\"\"Plot smoothed loss curves to better visualize trends\"\"\"\n",
        "        window = min(15, len(self.train_loss) // 3)\n",
        "        if window % 2 == 0:\n",
        "            window += 1\n",
        "\n",
        "        smoothed_train = savgol_filter(self.train_loss, window, 3)\n",
        "        smoothed_val = savgol_filter(self.val_loss, window, 3)\n",
        "\n",
        "        ax.plot(self.epochs, self.train_loss, 'lightblue', label='Training Loss (Raw)', alpha=0.3)\n",
        "        ax.plot(self.epochs, self.val_loss, 'lightcoral', label='Validation Loss (Raw)', alpha=0.3)\n",
        "        ax.plot(self.epochs, smoothed_train, 'blue', label='Training Loss (Smoothed)')\n",
        "        ax.plot(self.epochs, smoothed_val, 'red', label='Validation Loss (Smoothed)')\n",
        "\n",
        "        # Find divergence point\n",
        "        diff = np.array(self.val_loss) - np.array(self.train_loss)\n",
        "        divergence_point = np.where(diff > np.mean(diff) + np.std(diff))[0]\n",
        "        if len(divergence_point) > 0:\n",
        "            div_epoch = divergence_point[0]\n",
        "            ax.axvline(x=div_epoch, color='gray', linestyle='--', alpha=0.5)\n",
        "            ax.text(div_epoch + 0.5, max(self.train_loss), f'Divergence at epoch {div_epoch}')\n",
        "\n",
        "        ax.set_title('Loss Curves (Raw and Smoothed)')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    def _plot_accuracy_comparison(self, ax):\n",
        "        \"\"\"Plot accuracy curves with optimal stopping point\"\"\"\n",
        "        ax.plot(self.epochs, self.train_acc, 'b-', label='Training Accuracy')\n",
        "        ax.plot(self.epochs, self.val_acc, 'r-', label='Validation Accuracy')\n",
        "\n",
        "        # Find optimal accuracy point\n",
        "        best_val_epoch = np.argmax(self.val_acc) + 1\n",
        "        ax.axvline(x=best_val_epoch, color='green', linestyle='--', alpha=0.5)\n",
        "        ax.text(best_val_epoch + 0.5, max(self.train_acc), f'Best validation at epoch {best_val_epoch}')\n",
        "\n",
        "        ax.set_title('Accuracy Progression')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Accuracy (%)')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    def _plot_generalization_gap(self, ax):\n",
        "        \"\"\"Plot the generalization gap (difference between train and validation metrics)\"\"\"\n",
        "        acc_gap = np.array(self.train_acc) - np.array(self.val_acc)\n",
        "        loss_gap = np.array(self.val_loss) - np.array(self.train_loss)\n",
        "\n",
        "        ax.plot(self.epochs, acc_gap, 'b-', label='Accuracy Gap')\n",
        "        ax.plot(self.epochs, loss_gap, 'r-', label='Loss Gap')\n",
        "\n",
        "        # Find point where gap starts growing consistently\n",
        "        window = 5\n",
        "        gap_gradient = np.gradient(savgol_filter(acc_gap, window, 3))\n",
        "        consistent_growth = np.where(gap_gradient > np.mean(gap_gradient) + np.std(gap_gradient))[0]\n",
        "\n",
        "        if len(consistent_growth) > 0:\n",
        "            growth_point = consistent_growth[0]\n",
        "            ax.axvline(x=growth_point, color='red', linestyle='--', alpha=0.5)\n",
        "            ax.text(growth_point + 0.5, max(acc_gap), f'Gap growth at epoch {growth_point}')\n",
        "\n",
        "        ax.set_title('Generalization Gap')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Gap (Train - Validation)')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    def _plot_loss_gradient(self, ax):\n",
        "        \"\"\"Plot loss gradients to identify plateaus and unstable regions\"\"\"\n",
        "        val_gradient = np.gradient(self.val_loss)\n",
        "        train_gradient = np.gradient(self.train_loss)\n",
        "\n",
        "        ax.plot(self.epochs, train_gradient, 'b-', label='Training Loss Gradient')\n",
        "        ax.plot(self.epochs, val_gradient, 'r-', label='Validation Loss Gradient')\n",
        "\n",
        "        # Find plateau points\n",
        "        plateau_threshold = 0.001\n",
        "        plateau_points = np.where(np.abs(val_gradient) < plateau_threshold)[0]\n",
        "\n",
        "        if len(plateau_points) > 0:\n",
        "            first_plateau = plateau_points[0]\n",
        "            ax.axvline(x=first_plateau, color='purple', linestyle='--', alpha=0.5)\n",
        "            ax.text(first_plateau + 0.5, max(train_gradient), f'First plateau at epoch {first_plateau}')\n",
        "\n",
        "        ax.set_title('Loss Gradients')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Gradient')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    def get_stopping_recommendations(self):\n",
        "        \"\"\"Provide stopping recommendations based on multiple criteria\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # 1. Best validation accuracy\n",
        "        best_val_epoch = np.argmax(self.val_acc) + 1\n",
        "        recommendations.append(f\"Best validation accuracy achieved at epoch {best_val_epoch}\")\n",
        "\n",
        "        # 2. Generalization gap analysis\n",
        "        acc_gap = np.array(self.train_acc) - np.array(self.val_acc)\n",
        "        gap_threshold = np.mean(acc_gap) + np.std(acc_gap)\n",
        "        overfitting_epochs = np.where(acc_gap > gap_threshold)[0]\n",
        "        if len(overfitting_epochs) > 0:\n",
        "            recommendations.append(f\"Significant generalization gap detected at epoch {overfitting_epochs[0]}\")\n",
        "\n",
        "        # 3. Loss plateau detection\n",
        "        val_gradient = np.gradient(self.val_loss)\n",
        "        plateau_threshold = 0.001\n",
        "        plateau_points = np.where(np.abs(val_gradient) < plateau_threshold)[0]\n",
        "        if len(plateau_points) > 0:\n",
        "            recommendations.append(f\"Loss plateau detected at epoch {plateau_points[0]}\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "# Example usage:\n",
        "def analyze_training(history):\n",
        "    analyzer = TrainingAnalyzer(history)\n",
        "\n",
        "    # Plot comprehensive analysis\n",
        "    analyzer.plot_comprehensive_analysis()\n",
        "    plt.show()\n",
        "\n",
        "    # Print recommendations\n",
        "    print(\"\\nStopping Recommendations:\")\n",
        "    for rec in analyzer.get_stopping_recommendations():\n",
        "        print(f\"- {rec}\")\n",
        "\n",
        "# Add this to your training loop:\n",
        "history = train_model_enhanced(model, train_loader, val_loader, criterion, optimizer, scheduler)\n",
        "analyze_training(history)"
      ],
      "metadata": {
        "id": "iz92V6-r-M4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Performance Analysis and Improvement Recommendations"
      ],
      "metadata": {
        "id": "rnE-34LY-o50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "class ClassPerformanceAnalyzer:\n",
        "    def __init__(self, model, test_loader, classes, device):\n",
        "        self.model = model\n",
        "        self.test_loader = test_loader\n",
        "        self.classes = classes\n",
        "        self.device = device\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "        self.predictions = []\n",
        "        self.true_labels = []\n",
        "\n",
        "    def analyze(self):\n",
        "        \"\"\"Run complete analysis of class performance\"\"\"\n",
        "        self.model.eval()\n",
        "        confusion_mat = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in self.test_loader:\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Store predictions and true labels\n",
        "                self.predictions.extend(predicted.cpu().numpy())\n",
        "                self.true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update per-class accuracy\n",
        "                c = (predicted == labels).squeeze()\n",
        "                for i in range(len(labels)):\n",
        "                    label = labels[i]\n",
        "                    self.class_correct[label] += c[i].item()\n",
        "                    self.class_total[label] += 1\n",
        "\n",
        "        return self._generate_performance_report()\n",
        "\n",
        "    def _generate_performance_report(self):\n",
        "        \"\"\"Generate comprehensive performance report\"\"\"\n",
        "        # Calculate per-class accuracy\n",
        "        class_accuracy = {}\n",
        "        for i in range(len(self.classes)):\n",
        "            accuracy = 100 * self.class_correct[i] / self.class_total[i]\n",
        "            class_accuracy[self.classes[i]] = accuracy\n",
        "\n",
        "        # Sort classes by performance\n",
        "        sorted_classes = sorted(class_accuracy.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        conf_matrix = confusion_matrix(self.true_labels, self.predictions)\n",
        "\n",
        "        return {\n",
        "            'class_accuracy': class_accuracy,\n",
        "            'sorted_performance': sorted_classes,\n",
        "            'confusion_matrix': conf_matrix,\n",
        "            'classification_report': classification_report(\n",
        "                self.true_labels,\n",
        "                self.predictions,\n",
        "                target_names=self.classes\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def plot_performance_analysis(self, results):\n",
        "        \"\"\"Create visualization of class performance\"\"\"\n",
        "        fig = plt.figure(figsize=(20, 10))\n",
        "\n",
        "        # 1. Class Accuracy Bar Plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        accuracies = [acc for _, acc in results['sorted_performance']]\n",
        "        classes = [cls for cls, _ in results['sorted_performance']]\n",
        "        colors = ['green' if acc >= 75 else 'orange' if acc >= 60 else 'red' for acc in accuracies]\n",
        "\n",
        "        plt.bar(classes, accuracies, color=colors)\n",
        "        plt.title('Per-Class Accuracy')\n",
        "        plt.xlabel('Classes')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Add horizontal lines for reference\n",
        "        plt.axhline(y=75, color='g', linestyle='--', alpha=0.3, label='Good (75%)')\n",
        "        plt.axhline(y=60, color='orange', linestyle='--', alpha=0.3, label='Fair (60%)')\n",
        "        plt.legend()\n",
        "\n",
        "        # 2. Confusion Matrix Heatmap\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.heatmap(results['confusion_matrix'],\n",
        "                   annot=True,\n",
        "                   fmt='d',\n",
        "                   cmap='Blues',\n",
        "                   xticklabels=self.classes,\n",
        "                   yticklabels=self.classes)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def generate_improvement_recommendations(self, results):\n",
        "        \"\"\"Generate specific recommendations for worst performing classes\"\"\"\n",
        "        recommendations = {}\n",
        "        for class_name, accuracy in results['sorted_performance'][-3:]:  # Bottom 3 classes\n",
        "            conf_matrix = results['confusion_matrix']\n",
        "            class_idx = self.classes.index(class_name)\n",
        "\n",
        "            # Find most common misclassifications\n",
        "            misclassifications = [\n",
        "                (self.classes[i], conf_matrix[class_idx][i])\n",
        "                for i in range(len(self.classes))\n",
        "                if i != class_idx and conf_matrix[class_idx][i] > 0\n",
        "            ]\n",
        "            misclassifications.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            recommendations[class_name] = {\n",
        "                'accuracy': accuracy,\n",
        "                'common_confusions': misclassifications[:3],\n",
        "                'specific_recommendations': self._get_specific_recommendations(\n",
        "                    class_name,\n",
        "                    misclassifications\n",
        "                )\n",
        "            }\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def _get_specific_recommendations(self, class_name, confusions):\n",
        "        \"\"\"Generate specific recommendations based on confusion patterns\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # Data-related recommendations\n",
        "        recommendations.append({\n",
        "            'category': 'Data Enhancement',\n",
        "            'suggestions': [\n",
        "                f\"Collect more diverse {class_name} images\",\n",
        "                f\"Apply targeted augmentation for {class_name} characteristics\",\n",
        "                \"Use stratified sampling to balance class representation\"\n",
        "            ]\n",
        "        })\n",
        "\n",
        "        # Model-related recommendations\n",
        "        recommendations.append({\n",
        "            'category': 'Model Adjustments',\n",
        "            'suggestions': [\n",
        "                \"Implement class-weighted loss function\",\n",
        "                f\"Add attention mechanisms to focus on {class_name} distinctive features\",\n",
        "                \"Fine-tune model layers specifically for problematic classes\"\n",
        "            ]\n",
        "        })\n",
        "\n",
        "        # Training-related recommendations\n",
        "        recommendations.append({\n",
        "            'category': 'Training Strategy',\n",
        "            'suggestions': [\n",
        "                \"Use curriculum learning starting with easy examples\",\n",
        "                \"Implement hard negative mining\",\n",
        "                \"Apply mixup augmentation for confused classes\"\n",
        "            ]\n",
        "        })\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "def main():\n",
        "    # Example usage with your existing model and data\n",
        "    analyzer = ClassPerformanceAnalyzer(model, test_loader, classes, device)\n",
        "    results = analyzer.analyze()\n",
        "\n",
        "    # Plot performance analysis\n",
        "    analyzer.plot_performance_analysis(results)\n",
        "    plt.show()\n",
        "\n",
        "    # Get recommendations for worst performing classes\n",
        "    recommendations = analyzer.generate_improvement_recommendations(results)\n",
        "\n",
        "    # Print detailed analysis\n",
        "    print(\"\\nClass Performance Analysis:\")\n",
        "    print(\"-\" * 50)\n",
        "    for class_name, accuracy in results['sorted_performance']:\n",
        "        print(f\"{class_name:10s}: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nDetailed Recommendations for Worst Performing Classes:\")\n",
        "    print(\"-\" * 50)\n",
        "    for class_name, rec in recommendations.items():\n",
        "        print(f\"\\nClass: {class_name} (Accuracy: {rec['accuracy']:.2f}%)\")\n",
        "        print(\"Common confusions:\")\n",
        "        for confused_class, count in rec['common_confusions']:\n",
        "            print(f\"- Often confused with {confused_class} ({count} instances)\")\n",
        "\n",
        "        print(\"\\nImprovement Recommendations:\")\n",
        "        for category in rec['specific_recommendations']:\n",
        "            print(f\"\\n{category['category']}:\")\n",
        "            for suggestion in category['suggestions']:\n",
        "                print(f\"- {suggestion}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7rkb830Z-WsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5ytKXdbA-WHo"
      }
    }
  ]
}